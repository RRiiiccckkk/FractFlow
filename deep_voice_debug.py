#!/usr/bin/env python3
"""
æ·±åº¦è¯­éŸ³è°ƒè¯•å·¥å…·
æ£€æŸ¥éŸ³é¢‘è¾“å…¥çº§åˆ«ã€æœåŠ¡å™¨å“åº”å’ŒVADæ£€æµ‹é—®é¢˜
"""

import asyncio
import json
import os
import sys
import numpy as np
import base64
import threading
import queue
import time

sys.path.append(os.path.dirname(os.path.abspath(__file__)))

try:
    import pyaudio
    HAS_PYAUDIO = True
except ImportError:
    HAS_PYAUDIO = False

from tools.core.qwen_realtime_voice.qwen_realtime_voice_mcp import QwenRealtimeVoiceClient

class DeepDebugVoiceClient(QwenRealtimeVoiceClient):
    """æ·±åº¦è°ƒè¯•ç‰ˆè¯­éŸ³å®¢æˆ·ç«¯"""
    
    def __init__(self, api_key=None):
        super().__init__(api_key)
        self.audio_levels = []
        self.server_responses = []
        self.vad_events = []
        self.debug_mode = True
        
    def _recording_worker(self):
        """å¢å¼ºçš„å½•éŸ³å·¥ä½œçº¿ç¨‹ - åŒ…å«éŸ³é¢‘çº§åˆ«ç›‘æ§"""
        print("ğŸµ è°ƒè¯•å½•éŸ³çº¿ç¨‹å¯åŠ¨")
        
        chunk_count = 0
        total_volume = 0
        max_volume = 0
        
        while self.is_recording and not self.stop_event.is_set():
            try:
                # è¯»å–éŸ³é¢‘æ•°æ®
                audio_data = self.input_stream.read(1024, exception_on_overflow=False)
                
                # åˆ†æéŸ³é¢‘çº§åˆ«
                audio_array = np.frombuffer(audio_data, dtype=np.int16)
                if len(audio_array) > 0:
                    volume = np.sqrt(np.mean(audio_array.astype(np.float64)**2))
                    if not np.isnan(volume):
                        total_volume += volume
                        max_volume = max(max_volume, volume)
                        self.audio_levels.append(volume)
                        
                        # å®æ—¶æ˜¾ç¤ºéŸ³é¢‘çº§åˆ«
                        if chunk_count % 20 == 0:  # æ¯20ä¸ªå—æ˜¾ç¤ºä¸€æ¬¡
                            bar_length = int(min(volume / 100, 50))
                            bar = "â–ˆ" * bar_length + "â–‘" * (50 - bar_length)
                            avg_volume = total_volume / (chunk_count + 1) if chunk_count > 0 else 0
                            print(f"\rğŸ¤ éŸ³é‡: [{bar}] å½“å‰:{volume:.0f} å¹³å‡:{avg_volume:.0f} æœ€é«˜:{max_volume:.0f}", end="", flush=True)
                
                # æ”¾å…¥é˜Ÿåˆ—
                if not self.audio_queue.full():
                    self.audio_queue.put(audio_data)
                
                chunk_count += 1
                time.sleep(0.01)
                
            except Exception as e:
                if self.is_recording:
                    print(f"\nâŒ å½•éŸ³é”™è¯¯: {e}")
                break
                
        print(f"\nğŸ›‘ è°ƒè¯•å½•éŸ³çº¿ç¨‹ç»“æŸ - å¤„ç†äº† {chunk_count} ä¸ªéŸ³é¢‘å—")
        print(f"ğŸ“Š éŸ³é¢‘ç»Ÿè®¡: å¹³å‡éŸ³é‡ {total_volume/max(chunk_count,1):.0f}, æœ€é«˜éŸ³é‡ {max_volume:.0f}")
    
    async def _handle_responses(self):
        """å¢å¼ºçš„å“åº”å¤„ç† - è¯¦ç»†è®°å½•æ‰€æœ‰æœåŠ¡å™¨äº‹ä»¶"""
        try:
            message = await asyncio.wait_for(self.websocket.recv(), timeout=0.01)
            response_data = json.loads(message)
            
            response_type = response_data.get("type", "")
            timestamp = time.strftime("%H:%M:%S")
            
            # è®°å½•æ‰€æœ‰æœåŠ¡å™¨å“åº”
            self.server_responses.append({
                "timestamp": timestamp,
                "type": response_type,
                "data": response_data
            })
            
            # è¯¦ç»†æ‰“å°é‡è¦äº‹ä»¶
            if response_type == "input_audio_buffer.speech_started":
                print(f"\nğŸ¤ [{timestamp}] æ£€æµ‹åˆ°è¯­éŸ³å¼€å§‹!")
                self.vad_events.append({"timestamp": timestamp, "event": "speech_started"})
                if self.is_ai_speaking:
                    await self._interrupt_ai_response()
                    
            elif response_type == "input_audio_buffer.speech_stopped":
                print(f"\nğŸ”‡ [{timestamp}] æ£€æµ‹åˆ°è¯­éŸ³ç»“æŸ")
                self.vad_events.append({"timestamp": timestamp, "event": "speech_stopped"})
                self.interrupt_detected = False
                
            elif response_type == "input_audio_buffer.committed":
                print(f"\nğŸ“ [{timestamp}] éŸ³é¢‘ç¼“å†²åŒºå·²æäº¤")
                
            elif response_type == "conversation.item.created":
                item = response_data.get("item", {})
                if item.get("type") == "message":
                    role = item.get("role", "unknown")
                    print(f"\nğŸ’¬ [{timestamp}] åˆ›å»º{role}æ¶ˆæ¯")
                    
            elif response_type == "response.created":
                print(f"\nğŸ¤– [{timestamp}] AIå¼€å§‹å“åº”")
                
            elif response_type == "response.audio_transcript.delta":
                transcript = response_data.get("delta", "")
                if transcript.strip():
                    print(f"ğŸ’¬ AI: {transcript}", end="", flush=True)
                    
            elif response_type == "response.audio_transcript.done":
                print()  # æ¢è¡Œ
                
            elif response_type == "response.audio.delta":
                audio_base64 = response_data.get("delta", "")
                if audio_base64 and not self.interrupt_detected:
                    self.audio_buffer.put(audio_base64)
                    self.is_ai_speaking = True
                    
            elif response_type == "response.done":
                print(f"\nâœ… [{timestamp}] AIå“åº”å®Œæˆ")
                self.is_ai_speaking = False
                
            elif response_type == "error":
                error_info = response_data.get("error", {})
                print(f"\nâŒ [{timestamp}] æœåŠ¡å™¨é”™è¯¯: {error_info}")
                
            else:
                # è®°å½•å…¶ä»–äº‹ä»¶ä½†ä¸æ‰“å°è¯¦ç»†ä¿¡æ¯
                print(f"\nğŸ“¡ [{timestamp}] {response_type}")
                
        except asyncio.TimeoutError:
            pass
        except Exception as e:
            if "websocket" not in str(e).lower():
                print(f"\nâŒ å“åº”å¤„ç†é”™è¯¯: {e}")
    
    def get_debug_report(self):
        """ç”Ÿæˆè°ƒè¯•æŠ¥å‘Š"""
        print("\n" + "="*60)
        print("ğŸ“Š è°ƒè¯•æŠ¥å‘Š")
        print("="*60)
        
        # éŸ³é¢‘ç»Ÿè®¡
        if self.audio_levels:
            avg_level = np.mean(self.audio_levels)
            max_level = np.max(self.audio_levels)
            active_chunks = len([l for l in self.audio_levels if l > 100])
            
            print(f"ğŸ¤ éŸ³é¢‘ç»Ÿè®¡:")
            print(f"   å¹³å‡éŸ³é‡: {avg_level:.0f}")
            print(f"   æœ€é«˜éŸ³é‡: {max_level:.0f}")
            print(f"   æ´»è·ƒéŸ³é¢‘å—: {active_chunks}/{len(self.audio_levels)} ({active_chunks/len(self.audio_levels)*100:.1f}%)")
            
            if max_level < 50:
                print("   âš ï¸ éŸ³é‡è¿‡ä½ - å¯èƒ½éº¦å…‹é£æœ‰é—®é¢˜")
            elif active_chunks < len(self.audio_levels) * 0.1:
                print("   âš ï¸ è¯­éŸ³æ´»åŠ¨å¾ˆå°‘ - å°è¯•æ›´å¤§å£°è¯´è¯")
            else:
                print("   âœ… éŸ³é¢‘è¾“å…¥æ­£å¸¸")
        
        # VADäº‹ä»¶ç»Ÿè®¡
        print(f"\nğŸ” VADäº‹ä»¶: {len(self.vad_events)} ä¸ª")
        for event in self.vad_events[-5:]:  # æ˜¾ç¤ºæœ€è¿‘5ä¸ªäº‹ä»¶
            print(f"   {event['timestamp']}: {event['event']}")
        
        # æœåŠ¡å™¨å“åº”ç»Ÿè®¡
        response_types = {}
        for resp in self.server_responses:
            resp_type = resp["type"]
            response_types[resp_type] = response_types.get(resp_type, 0) + 1
        
        print(f"\nğŸ“¡ æœåŠ¡å™¨å“åº”ç»Ÿè®¡:")
        for resp_type, count in response_types.items():
            print(f"   {resp_type}: {count} æ¬¡")
        
        # é—®é¢˜è¯Šæ–­
        print(f"\nğŸ”§ é—®é¢˜è¯Šæ–­:")
        if len(self.vad_events) == 0:
            print("   âŒ æœåŠ¡å™¨æœªæ£€æµ‹åˆ°ä»»ä½•è¯­éŸ³æ´»åŠ¨")
            print("   ğŸ’¡ å¯èƒ½åŸå› :")
            print("      â€¢ VADé˜ˆå€¼è®¾ç½®è¿‡é«˜")
            print("      â€¢ éŸ³é¢‘æ ¼å¼ä¸åŒ¹é…")
            print("      â€¢ éº¦å…‹é£æƒé™é—®é¢˜")
            print("      â€¢ ç½‘ç»œä¼ è¾“é—®é¢˜")
        else:
            print("   âœ… æœåŠ¡å™¨æ£€æµ‹åˆ°è¯­éŸ³æ´»åŠ¨")

async def run_deep_debug_test():
    """è¿è¡Œæ·±åº¦è°ƒè¯•æµ‹è¯•"""
    print("ğŸ” æ·±åº¦è¯­éŸ³è°ƒè¯•æµ‹è¯•")
    print("="*50)
    
    client = DeepDebugVoiceClient()
    
    try:
        # è¿æ¥
        print("ğŸ”— è¿æ¥API...")
        connect_result = await client.connect()
        if not connect_result["success"]:
            print(f"âŒ è¿æ¥å¤±è´¥: {connect_result['error']}")
            return
        
        print(f"âœ… è¿æ¥æˆåŠŸ: {connect_result['session_id']}")
        
        # åˆå§‹åŒ–éŸ³é¢‘ - å¼ºåˆ¶ä½¿ç”¨å†…ç½®éº¦å…‹é£
        client.pyaudio_instance = pyaudio.PyAudio()
        
        # æ‰¾åˆ°MacBookå†…ç½®éº¦å…‹é£
        device_id = None
        for i in range(client.pyaudio_instance.get_device_count()):
            info = client.pyaudio_instance.get_device_info_by_index(i)
            if "MacBook" in info['name'] and "Microphone" in info['name']:
                device_id = i
                break
        
        if device_id is None:
            print("âŒ æœªæ‰¾åˆ°MacBookå†…ç½®éº¦å…‹é£")
            return
        
        print(f"ğŸ¤ ä½¿ç”¨è®¾å¤‡: {client.pyaudio_instance.get_device_info_by_index(device_id)['name']}")
        
        # åˆ›å»ºè¾“å…¥æµ
        client.input_stream = client.pyaudio_instance.open(
            format=pyaudio.paInt16,
            channels=1,
            rate=16000,  # å¼ºåˆ¶ä½¿ç”¨16kHz
            input=True,
            input_device_index=device_id,
            frames_per_buffer=1024
        )
        
        client.is_recording = True
        client.stop_event.clear()
        
        # å¯åŠ¨çº¿ç¨‹
        client.recording_thread = threading.Thread(target=client._recording_worker)
        client.recording_thread.start()
        
        client.processing_thread = threading.Thread(target=client._processing_worker)
        client.processing_thread.start()
        
        print("\n" + "="*60)
        print("ğŸ¤ æ·±åº¦è°ƒè¯•æ¨¡å¼å·²å¯åŠ¨")
        print("ğŸ“Š å®æ—¶æ˜¾ç¤ºéŸ³é¢‘çº§åˆ«å’ŒæœåŠ¡å™¨å“åº”")
        print("ğŸ—£ï¸ è¯·å¤§å£°æ¸…æ™°åœ°è¯´è¯:")
        print("   â€¢ 'ä½ å¥½åƒé—®' - æµ‹è¯•åŸºç¡€è¯†åˆ«")
        print("   â€¢ 'ç°åœ¨å‡ ç‚¹äº†' - æµ‹è¯•é—®ç­”")
        print("   â€¢ è§‚å¯ŸéŸ³é‡æ¡å’ŒæœåŠ¡å™¨å“åº”")
        print("="*60)
        
        # è¿è¡Œ30ç§’æµ‹è¯•
        for i in range(30):
            await asyncio.sleep(1)
            print(f"\râ±ï¸ æµ‹è¯•æ—¶é—´: {30-i}ç§’  ", end="", flush=True)
        
        print("\n\nğŸ›‘ æµ‹è¯•ç»“æŸ")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•é”™è¯¯: {e}")
    
    finally:
        # ç”Ÿæˆè°ƒè¯•æŠ¥å‘Š
        client.get_debug_report()
        
        # æ¸…ç†
        client.is_recording = False
        client.stop_event.set()
        
        if client.recording_thread:
            client.recording_thread.join(timeout=2)
        if client.processing_thread:
            client.processing_thread.join(timeout=2)
        
        if client.input_stream:
            client.input_stream.close()
        if client.pyaudio_instance:
            client.pyaudio_instance.terminate()
        
        await client.disconnect()
        print("\nâœ… è°ƒè¯•æµ‹è¯•å®Œæˆ")

if __name__ == "__main__":
    asyncio.run(run_deep_debug_test()) 